{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b923d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始篇數：35\n",
      "重複篇數：4\n",
      "\n",
      "重複條目：\n",
      " - 借名登記之相關法律效力－最高法院 108 年度台上大字第 1652 號裁定 / 王怡蘋 / 2022\n",
      " - 原住民保留地「借名登記契約」的效力－公法規定的私法效力，或私法自治的公法限制？ / 李建良 / 2021\n",
      " - 不動產借名登記契約有效性的檢討 / 黃健彰 / 2019\n",
      " - 不動產借名登記契約之負擔行為與處分行為 / 蔡旻耿 and 陳靜瑩 / 2016\n",
      "\n",
      "去重後篇數：31\n",
      "去重後條目：\n",
      " - 不動產借名登記相關民事法律關係－以評析近年最高法院裁判為主 / 黃健彰 / 2025\n",
      " - 民事法裁判精選－借名登記之內部關係與外部關係（111 台上 2686 判決） / 顏佑紘 / 2024\n",
      " - 論借名登記 / 蔣瑞安 / 2023\n",
      " - 借名登記股份的回復與公同共有－最高法院 110 年度台上字第 724 號民事判決 / 陳榮傳 / 2022\n",
      " - 原住民保留地的借名登記－大法庭裁定的商榷 / 陳榮傳 / 2022\n",
      " - 自耕能力與借名登記－最高法院 107 年度台上字第 1023 號判決評析 / 林慶郎 / 2022\n",
      " - 借名登記之舉證責任 / 鄭冠宇 / 2022\n",
      " - 借名登記之相關法律效力－最高法院 108 年度台上大字第 1652 號裁定 / 王怡蘋 / 2022\n",
      " - 原住民保留地借名登記之法律關係－兼評最高法院 108 年台上大字第 1636 號民事裁定 / 蔡旻耿 / 2022\n",
      " - 原住民保留地「借名登記契約」的效力－公法規定的私法效力，或私法自治的公法限制？ / 李建良 / 2021\n",
      " - 農地借名登記問題探討－以刪除前土地法第 30 條適用範圍及當事人間契約有效性為核心 / 李維中 / 2021\n",
      " - 淺談商標借名登記 / 涂軼 / 2020\n",
      " - 不動產借名登記契約有效性的檢討 / 黃健彰 / 2019\n",
      " - 我國不動產借名登記後處分之效力問題分析－以最高法院 106 年第 3 次民事庭會議決議為中心 / 陳明燦 / 2018\n",
      " - 借名登記契約是否屬「因委任事務之性質不能消滅者」之實務爭議－最高法院 106 年度台上字第 410 號民事判決評析 / 邱玟惠 / 2018\n",
      " - 借名登記對外效力之探討－法學方法與債法現代化之思考 / 林大洋 / 2017\n",
      " - 借名登記財產之請求返還方式 / 林誠二 / 2017\n",
      " - 原借名登記因死亡消滅，繼承人未辦繼承登記前借名登記契約無效？－簡評最高法院 106 台上 1222 號判決 / 蔡瑞麟 / 2017\n",
      " - 借名登記契約當事人死亡之實務爭議－最高法院 104 年度台上字第 1787 號民事判決評析 / 邱玟惠 / 2016\n",
      " - 不動產借名登記契約之負擔行為與處分行為 / 蔡旻耿 and 陳靜瑩 / 2016\n",
      " - 我國不動產借名登記契約之發展現狀－特別著重觀察內部效力與外部效力演變之互動 / 吳從周 / 2015\n",
      " - 真正創作人得就專利申請權與他人成立借名登記契約 / 簡秀如 and 吳詩儀 / 2015\n",
      " - 專利與借名登記 / 蔣文正 / 2015\n",
      " - 專利證書所示之專利權人是否真為實質權利人？—專利權人之借名登記 / 簡秀如 and 吳詩儀 / 2015\n",
      " - 論借名登記契約／最高院 100 台上 2101 判決 / 蔡晶瑩 / 2014\n",
      " - 由借名登記契約論不動產物權變動登記之效力 / 林誠二 / 2013\n",
      " - 談借名登記契約－兼評最高法院 98 年度台上字第 990 號判決 / 郭松濤 / 2012\n",
      " - 借名登記之名消極信託之實－評最高法院九十八年度台上字第七六號判決 / 謝哲勝 / 2010\n",
      " - 消極信託和借名登記形同脫法行為－實務相關判決評釋 / 謝哲勝 / 2006\n",
      " - 脫法行為、消極信託及借名登記契約－最高法院九十四年度台上字第三六二號民事判決評釋 / 陳聰富 / 2005\n",
      " - 借名登記是信託且可能無效－以最高法院 107 年度台上字第 792 號民事判決為例 / 游進發 / 2020\n",
      "\n",
      "✅ 已輸出去重後檔案：papers_dedup.bib\n",
      "\n",
      "✅ 已輸出 howpublish DOCX（共 31 筆）：【借名登記】文獻清單匯出.docx\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "from functools import lru_cache\n",
    "from pathlib import Path\n",
    "\n",
    "# === 使用者設定 ===\n",
    "INPUT_BIB_PATH = Path('papers.bib')\n",
    "DEDUP_BIB_PATH = Path('papers_dedup.bib')\n",
    "HOWPUBLISH_DOCX_PATH = Path('【借名登記】文獻清單匯出.docx')\n",
    "HOWPUBLISH_SORT_MODE = 'stroke'  # 'stroke' 或 'date'\n",
    "DATE_SORT_DESCENDING = True      # 僅在 HOWPUBLISH_SORT_MODE='date' 時生效\n",
    "STROKE_DATA_PATH = Path('reference/Unihan_IRGSources.txt')\n",
    "\n",
    "# === 工具函式 ===\n",
    "def clean_entry(entry: str) -> str:\n",
    "    \"\"\"刪除值為『未知』『無』或空 {} 的欄位\"\"\"\n",
    "    return re.sub(\n",
    "        r'^\\s*\\w+\\s*=\\s*\\{\\s*(?:未知|無)?\\s*\\},?,?\\s*$',\n",
    "        '',\n",
    "        entry,\n",
    "        flags=re.MULTILINE\n",
    "    )\n",
    "\n",
    "def parse_entry(entry: str):\n",
    "    \"\"\"解析 Bib 條目\"\"\"\n",
    "    m_type = re.match(r'(\\w+)\\s*\\{', entry)\n",
    "    m_title = re.search(r'title\\s*=\\s*\\{(.*?)\\}', entry, re.DOTALL)\n",
    "    m_author = re.search(r'author\\s*=\\s*\\{(.*?)\\}', entry, re.DOTALL)\n",
    "    m_year = re.search(r'year\\s*=\\s*\\{(.*?)\\}', entry)\n",
    "    typ = m_type.group(1) if m_type else 'unknown'\n",
    "    title = m_title.group(1).strip() if m_title else '未知標題'\n",
    "    author = m_author.group(1).strip() if m_author else '未知作者'\n",
    "    year = m_year.group(1).strip() if m_year else '未知年份'\n",
    "    cleaned_entry = clean_entry(entry)\n",
    "    return typ, title, author, year, cleaned_entry\n",
    "\n",
    "def extract_field(entry: str, field_name: str):\n",
    "    pattern = rf'{field_name}\\s*=\\s*\\{{(.*?)\\}}'\n",
    "    match = re.search(pattern, entry, flags=re.DOTALL | re.IGNORECASE)\n",
    "    if match:\n",
    "        value = match.group(1).strip()\n",
    "        return value or None\n",
    "    return None\n",
    "\n",
    "def parse_date_from_text(text: str) -> datetime:\n",
    "    match = re.search(r'(\\d{4})年\\s*(\\d{1,2})?月?\\s*(\\d{1,2})?日?', text)\n",
    "    if not match:\n",
    "        return datetime.min\n",
    "    year = int(match.group(1))\n",
    "    month = int(match.group(2)) if match.group(2) else 1\n",
    "    day = int(match.group(3)) if match.group(3) else 1\n",
    "    try:\n",
    "        return datetime(year, month, day)\n",
    "    except ValueError:\n",
    "        return datetime(year, month, 1)\n",
    "\n",
    "@lru_cache(maxsize=1)\n",
    "def load_stroke_map():\n",
    "    stroke_map = {}\n",
    "    path = STROKE_DATA_PATH\n",
    "    if not path.exists():\n",
    "        print(f'⚠️ 找不到字根筆畫檔案：{path}，將以字典順序排序作者。')\n",
    "        return stroke_map\n",
    "    with path.open('r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if '\\tkTotalStrokes\\t' not in line:\n",
    "                continue\n",
    "            code, field, value = line.strip().split('\\t')\n",
    "            if field != 'kTotalStrokes':\n",
    "                continue\n",
    "            primary = value.split()[0]\n",
    "            try:\n",
    "                cp = int(code[2:], 16)\n",
    "                stroke_map[chr(cp)] = int(primary)\n",
    "            except ValueError:\n",
    "                continue\n",
    "    return stroke_map\n",
    "\n",
    "def stroke_key(name: str):\n",
    "    stroke_map = load_stroke_map()\n",
    "    if not stroke_map:\n",
    "        return (name,)\n",
    "    counts = []\n",
    "    for ch in name:\n",
    "        if ch.isspace():\n",
    "            continue\n",
    "        counts.append(stroke_map.get(ch, 100))\n",
    "    return tuple(counts) if counts else (100,)\n",
    "\n",
    "def parse_howpublish_entry(text: str):\n",
    "    author_segment, _, _ = text.partition('，')\n",
    "    primary_author = author_segment.split('、')[0].strip() if author_segment else '未知作者'\n",
    "    date_value = parse_date_from_text(text)\n",
    "    return {\n",
    "        'raw': text,\n",
    "        'primary_author': primary_author,\n",
    "        'date': date_value,\n",
    "    }\n",
    "\n",
    "def collect_howpublish(entries):\n",
    "    values = []\n",
    "    for _, _, _, entry in entries:\n",
    "        howpublish = extract_field(entry, 'howpublished') or extract_field(entry, 'howpublish')\n",
    "        if howpublish:\n",
    "            values.append(parse_howpublish_entry(howpublish))\n",
    "    return values\n",
    "\n",
    "def sort_howpublish(entries):\n",
    "    mode = HOWPUBLISH_SORT_MODE.lower()\n",
    "    if mode == 'date':\n",
    "        return sorted(\n",
    "            entries,\n",
    "            key=lambda item: (item['date'], stroke_key(item['primary_author']), item['raw']),\n",
    "            reverse=DATE_SORT_DESCENDING\n",
    "        )\n",
    "    return sorted(\n",
    "        entries,\n",
    "        key=lambda item: (stroke_key(item['primary_author']), item['date'], item['raw'])\n",
    "    )\n",
    "\n",
    "def write_howpublish_docx(items, output_path: Path):\n",
    "    if not items:\n",
    "        print()\n",
    "        print('⚠️ 去重後條目未找到 howpublish 欄位，未產生 DOCX。')\n",
    "        return\n",
    "    md_path = output_path.with_suffix('.md')\n",
    "    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    sorted_items = sort_howpublish(items)\n",
    "    sort_label = '作者筆畫' if HOWPUBLISH_SORT_MODE.lower() == 'stroke' else '出版時間'\n",
    "    total_count = len(sorted_items)\n",
    "    lines = [\n",
    "        '# 文獻清單 匯出',\n",
    "        f'- 產出時間：{timestamp}',\n",
    "        f'- 排序方式：{sort_label}',\n",
    "        f'- 條目總數：{total_count}',\n",
    "        '',\n",
    "        '## 條目清單',\n",
    "        ''\n",
    "    ] + [f\"{idx_val}. {item['raw']}\" for idx_val, item in enumerate(sorted_items, 1)]\n",
    "\n",
    "    md_path.write_text('\\n'.join(lines), encoding='utf-8')\n",
    "\n",
    "    subprocess.run([\n",
    "        'pandoc',\n",
    "        str(md_path),\n",
    "        '-f', 'markdown',\n",
    "        '-t', 'docx',\n",
    "        '-o', str(output_path)\n",
    "    ], check=True)\n",
    "\n",
    "    print()\n",
    "    print(f\"✅ 已輸出 howpublish DOCX（共 {len(sorted_items)} 筆）：{output_path}\")\n",
    "\n",
    "# === 主流程 ===\n",
    "text = INPUT_BIB_PATH.read_text(encoding='utf-8')\n",
    "entries = re.split(r'@(?=\\w+\\s*\\{)', text)\n",
    "entries = [e.strip() for e in entries if e.strip()]\n",
    "parsed = [parse_entry(e) for e in entries]\n",
    "\n",
    "# === 去重 ===\n",
    "seen = {}\n",
    "duplicates = []\n",
    "for typ, title, author, year, entry in parsed:\n",
    "    key = title.lower()\n",
    "    if key in seen:\n",
    "        duplicates.append((title, author, year))\n",
    "    else:\n",
    "        seen[key] = (title, author, year, entry)\n",
    "unique_entries = list(seen.values())\n",
    "\n",
    "# === 統計輸出 ===\n",
    "total_count = len(parsed)\n",
    "dup_count = len(duplicates)\n",
    "unique_count = len(unique_entries)\n",
    "print(f'原始篇數：{total_count}')\n",
    "print(f'重複篇數：{dup_count}')\n",
    "if dup_count:\n",
    "    print()\n",
    "    print('重複條目：')\n",
    "    for title, author, year in duplicates:\n",
    "        print(f' - {title} / {author} / {year}')\n",
    "print()\n",
    "print(f'去重後篇數：{unique_count}')\n",
    "print('去重後條目：')\n",
    "for title, author, year, _ in unique_entries:\n",
    "    print(f' - {title} / {author} / {year}')\n",
    "\n",
    "# === 寫出去重後 Bib ===\n",
    "dedup_text = '\\n\\n'.join(f\"@{entry}\" for _, _, _, entry in unique_entries)\n",
    "DEDUP_BIB_PATH.write_text(dedup_text, encoding='utf-8')\n",
    "print()\n",
    "print(f\"✅ 已輸出去重後檔案：{DEDUP_BIB_PATH}\")\n",
    "\n",
    "# === 匯出 howpublish DOCX ===\n",
    "howpublish_items = collect_howpublish(unique_entries)\n",
    "write_howpublish_docx(howpublish_items, HOWPUBLISH_DOCX_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e109fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
