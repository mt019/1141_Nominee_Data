{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3db585d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š è‡ªå‹•å¾ CSV ç”Ÿæˆæ¸…å–®ï¼ˆä¾ custom_score é™åºï¼‰ã€‚\n",
      "å…±è¼‰å…¥ 271 å€‹æª”åã€‚\n",
      "ğŸ“š å·²è¼‰å…¥ 593 ç­† Bib æ¢ç›®ã€‚\n",
      "âœ… è¼¸å‡º 271 ç­† â†’ æŒ‰è¼¸å…¥é †åº.md\n",
      "âœ… è¼¸å‡º 271 ç­† â†’ æŒ‰ç­†ç•«æ’åº.md\n",
      "ğŸ¯ æ‰€æœ‰æª”æ¡ˆçš†æˆåŠŸåŒ¹é…ã€‚\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "import unicodedata\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# === å¯èª¿æ•´åƒæ•¸ ===\n",
    "root_dir = Path.cwd()             # bib æª”æ‰€åœ¨æ ¹ç›®éŒ„\n",
    "reference_file = Path(\"reference/Unihan_IRGSources.txt\")\n",
    "output_dir = Path(\"match_bib_output_md\")\n",
    "mode = 3                          # 1=ä¾è¼¸å…¥é †åºï¼›2=ä¾ç­†ç•«ï¼›3=å…©è€…éƒ½è¼¸å‡º\n",
    "prefix_digits = 3                 # æµæ°´è™Ÿä½æ•¸\n",
    "TOP_N = None                       # åªè¼¸å‡ºå‰ N ç­†ï¼ˆNone è¡¨ç¤ºå…¨éƒ¨ï¼‰\n",
    "CSV_PATH = Path(\"filtered_keywords_with_score.csv\")  # è‹¥ç„¡æ‰‹å‹•è²¼æ¸…å–®å‰‡è‡ªå‹•ä¾ CSV\n",
    "\n",
    "# === æ‰‹å‹•æ¸…å–®ï¼ˆç•™ç©ºå‰‡è‡ªå‹•ä½¿ç”¨ CSVï¼‰ ===\n",
    "file_names_text = \"\"\" \"\"\"\n",
    "\n",
    "# === æ¸…å–®ä¾†æº ===\n",
    "if file_names_text.strip():\n",
    "    print(\"ğŸ“„ ä½¿ç”¨æ‰‹å‹•è²¼ä¸Šçš„æª”åæ¸…å–®ã€‚\")\n",
    "    file_names = [x.strip() for x in file_names_text.splitlines() if x.strip()]\n",
    "else:\n",
    "    print(\"ğŸ“Š è‡ªå‹•å¾ CSV ç”Ÿæˆæ¸…å–®ï¼ˆä¾ custom_score é™åºï¼‰ã€‚\")\n",
    "    df = pd.read_csv(CSV_PATH)\n",
    "    if \"paper_name\" not in df.columns:\n",
    "        raise ValueError(\"CSV æª”ç¼ºå°‘æ¬„ä½ 'paper_name'\")\n",
    "    df_sorted = df.sort_values(\"custom_score\", ascending=False)\n",
    "    if TOP_N is not None:\n",
    "        df_sorted = df_sorted.head(TOP_N)\n",
    "        print(f\"âš™ï¸ åƒ…è¼¸å‡ºå‰ {TOP_N} ç¯‡ï¼ˆä¾ custom_score æ’åºï¼‰\")\n",
    "    file_names = list(df_sorted[\"paper_name\"])\n",
    "\n",
    "print(f\"å…±è¼‰å…¥ {len(file_names)} å€‹æª”åã€‚\")\n",
    "\n",
    "# === è¼‰å…¥ç­†ç•«è¡¨ ===\n",
    "def load_stroke_table(path):\n",
    "    table = {}\n",
    "    if not path.exists():\n",
    "        return table\n",
    "    pattern = re.compile(r\"U\\+([0-9A-F]+)\\s+kTotalStrokes\\s+(\\d+)\")\n",
    "    for line in path.read_text(encoding=\"utf-8\", errors=\"ignore\").splitlines():\n",
    "        m = pattern.search(line)\n",
    "        if m:\n",
    "            char = chr(int(m.group(1), 16))\n",
    "            table[char] = int(m.group(2))\n",
    "    return table\n",
    "\n",
    "strokes_table = load_stroke_table(reference_file)\n",
    "\n",
    "# === æ­£è¦åŒ– ===\n",
    "def normalize(s: str) -> str:\n",
    "    if not s:\n",
    "        return \"\"\n",
    "    s = re.sub(r\"[ \\u3000]\", \" \", s)\n",
    "    s = re.sub(r\"[â€“â€”ï¼]\", \"-\", s)\n",
    "    s = re.sub(r\"[ï¼šï¹•]\", \":\", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "# === ç­†ç•«æ•¸ ===\n",
    "def stroke_count(name: str) -> int:\n",
    "    count = 0\n",
    "    for ch in name:\n",
    "        if ch in strokes_table:\n",
    "            count += strokes_table[ch]\n",
    "        elif unicodedata.category(ch).startswith(\"Lo\"):\n",
    "            count += 10  # æœªçŸ¥å­—é è¨­10ç­†ç•«\n",
    "    return count\n",
    "\n",
    "# === è§£æ Bib ===\n",
    "def parse_bib_entry(text):\n",
    "    title_match = re.search(r\"title\\s*=\\s*\\{([^}]+)\\}\", text)\n",
    "    author_match = re.search(r\"author\\s*=\\s*\\{([^}]+)\\}\", text)\n",
    "    how_match = re.search(r\"howpublished\\s*=\\s*\\{([^}]+)\\}\", text)\n",
    "    if title_match:\n",
    "        title = normalize(title_match.group(1))\n",
    "        author = normalize(author_match.group(1)) if author_match else \"\"\n",
    "        how = normalize(how_match.group(1)) if how_match else \"\"\n",
    "        return title, author, how\n",
    "    return None, None, None\n",
    "\n",
    "# === æƒæ Bib ===\n",
    "bib_files = list(root_dir.rglob(\"*.bib\"))\n",
    "entries = []\n",
    "for bib_path in bib_files:\n",
    "    text = bib_path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    for raw in re.split(r\"@article|@book|@misc\", text)[1:]:\n",
    "        title, author, how = parse_bib_entry(raw)\n",
    "        if title:\n",
    "            entries.append((title, author, how))\n",
    "print(f\"ğŸ“š å·²è¼‰å…¥ {len(entries)} ç­† Bib æ¢ç›®ã€‚\")\n",
    "\n",
    "# === æ¯”å°æª”å ===\n",
    "matches = []\n",
    "for fname in file_names:\n",
    "    for title, author, how in entries:\n",
    "        if normalize(title).replace(\" \", \"\") in normalize(fname).replace(\" \", \"\"):\n",
    "            matches.append((fname, title, author, how))\n",
    "            break\n",
    "\n",
    "# === åŒ¯å‡ºçµæœ ===\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "def export_markdown(data, filename, title):\n",
    "    lines = [\n",
    "        f\"# {title}\",\n",
    "        \"\",\n",
    "        f\"**ç”Ÿæˆæ™‚é–“**ï¼š{timestamp}\",\n",
    "        \"\",\n",
    "        f\"æ­¤æª”æ¡ˆåˆ—å‡ºè‡ªå‹•æ¯”å°æ‰€å¾—æ–‡ç»å¼•ç”¨æ¸…å–®ï¼ˆå…± {len(data)} ç­†ï¼‰ã€‚\",\n",
    "        \"\",\n",
    "        \"ä»¥ä¸‹ä¾æŒ‡å®šé †åºç·¨è™Ÿï¼š\",\n",
    "        \"\"\n",
    "    ]\n",
    "    for i, (_, _, _, how) in enumerate(data, start=1):\n",
    "        num = f\"{i:0{prefix_digits}d}\"\n",
    "        lines.append(f\"{num}. {how}\")\n",
    "    (output_dir / filename).write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n",
    "    print(f\"âœ… è¼¸å‡º {len(data)} ç­† â†’ {filename}\")\n",
    "\n",
    "# === ä¸»è¼¸å‡º ===\n",
    "if mode in (1, 3):\n",
    "    export_markdown(matches, \"æŒ‰è¼¸å…¥é †åº.md\", \"æ–‡ç»æ¸…å–®ï¼ˆæŒ‰è¼¸å…¥é †åºï¼‰\")\n",
    "\n",
    "if mode in (2, 3):\n",
    "    sorted_matches = sorted(matches, key=lambda x: stroke_count(x[2]) if x[2] else 0)\n",
    "    export_markdown(sorted_matches, \"æŒ‰ç­†ç•«æ’åº.md\", \"æ–‡ç»æ¸…å–®ï¼ˆæŒ‰ä½œè€…ç­†ç•«æ’åºï¼‰\")\n",
    "\n",
    "# === æ‰¾å‡ºæœªåŒ¹é…é … ===\n",
    "matched_fnames = {m[0] for m in matches}\n",
    "unmatched = [f for f in file_names if f not in matched_fnames]\n",
    "\n",
    "if unmatched:\n",
    "    miss_path = output_dir / \"æœªåŒ¹é…æ¸…å–®.txt\"\n",
    "    miss_path.write_text(\"\\n\".join(unmatched), encoding=\"utf-8\")\n",
    "    print(f\"âš ï¸ æœªåŒ¹é… {len(unmatched)} ç­†ï¼Œå·²è¼¸å‡º â†’ {miss_path.resolve()}\")\n",
    "else:\n",
    "    print(\"ğŸ¯ æ‰€æœ‰æª”æ¡ˆçš†æˆåŠŸåŒ¹é…ã€‚\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
